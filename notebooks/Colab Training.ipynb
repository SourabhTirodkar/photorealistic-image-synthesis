{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN Colab Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_aiDJY9C0ZQ"
      },
      "source": [
        "Run the below cell and paste the contents in setup.sh. Remember to update the checkpoint directory with the drive folder. So it can be reloaded when colab session expires. Click [here](https://drive.google.com/file/d/1zH_5w_OSmGOygftjNtfvJJM1Tct9w2hm/view?usp=sharing) to download setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNRBrt5RQl4W"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGEVbZ9t99lo"
      },
      "source": [
        "!touch setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmODrXGqC7lK"
      },
      "source": [
        "Copy below cell contents and paste in setup.sh and run the shell file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_JqOkATH7O_"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "pip install tensorflow==1.15.0\n",
        "pip install scipy==1.2.0\n",
        "pip install pillow==6.1.0\n",
        "pip install imgaug==0.2.6\n",
        "pip install umap-learn==0.3.10\n",
        "pip install tensorflow-probability==0.7.0\n",
        "pip install gdown\n",
        "pip install pipdeptree\n",
        "\n",
        "gdown https://drive.google.com/uc?id=15WG2XJhJvWVD1yndqYwgxTlL0oOQ-g41\n",
        "tar -zvxf SPADE.tar.gz\n",
        "\n",
        "gdown https://drive.google.com/uc?id=1mEMQJrW9tshl-UeopWMYOWbEMo6WHRxM\n",
        "unzip -j Final_orginal_images.zip \"Final_orginal_images/*\" -d SPADE/datasets/Places365/train_B/\n",
        "\n",
        "cd SPADE/models/networks/\n",
        "git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
        "cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .\n",
        "cd ../../\n",
        "\n",
        "mv datasets/Places365/train_A/lake_00000030\\ .png datasets/Places365/train_A/lake_00000030.png\n",
        "pip install -r requirements.txt\n",
        "\n",
        "pwd\n",
        "\n",
        "python train.py \\\n",
        "    --name Places365 \\   \n",
        "    --dataset_mode custom \\    \n",
        "    --phase train \\    \n",
        "    --batchSize 2 \\    \n",
        "    --label_nc 150 \\    \n",
        "    --load_size 256 \\    \n",
        "    --crop_size 256 \\   \n",
        "    --ndf 128 \\    \n",
        "    --ngf 128 \\    \n",
        "    --save_epoch_freq 1 \\    \n",
        "    --no_pairing_check \\    \n",
        "    --preprocess_mode none \\    \n",
        "    --label_dir datasets/Places365/train_A \\    \n",
        "    --image_dir datasets/Places365/train_B \\    \n",
        "    --no_instance \\    \n",
        "    --cache_filelist_write \\    \n",
        "    --cache_filelist_read \\    \n",
        "    --tf_log \\    \n",
        "    --dataroot datasets/Places365 \\        \n",
        "    --beta1 0.5 \\\n",
        "    --beta2 0.99 \\   \n",
        "    --lr 9e-5 \\         \n",
        "    --use_vae \\\n",
        "    --nef 64 \\\n",
        "    --serial_batches \\\n",
        "    --checkpoints_dir /content/drive/MyDrive/Spade\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzjgC8JXCzvt"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FC6Qi9rL3t2"
      },
      "source": [
        "This cell should be executed **only if** you want to run the training manually again. Dont run otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b6IN_TqHYm-"
      },
      "source": [
        "%cd SPADE/\n",
        "!python train.py --name Places365 --dataset_mode custom --phase train --batchSize 2 --label_nc 150 --load_size 256 --crop_size 256 --ndf 88 --ngf 88 --norm_G spectralspadesyncbatch3x3  --save_epoch_freq 1 --no_pairing_check --preprocess_mode none --label_dir datasets/Places365/train_A --image_dir datasets/Places365/train_B --no_instance --cache_filelist_write --cache_filelist_read --tf_log --dataroot datasets/Places365 --lr 0.0003 --serial_batches --use_vae --nef 32 --checkpoints_dir ./checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G5g8lGw7Obm"
      },
      "source": [
        "If you would like to remove some categories from the train folder for quick debugging, you can run the below cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uM9vAs2G2GB",
        "outputId": "67b25a0e-2468-46f4-9e33-e427b7433efe"
      },
      "source": [
        "import os, glob\n",
        "\n",
        "count = 0\n",
        "for folder in [\"train_A\",\"train_B\"]:\n",
        "    for cat in [\"beach\",\"beach_house\", \"boathouse\", \"boat_deck\", \"lake\",\"river\", \"sky\", \"field_road\", \"ocean\",\"hot_spring\"]:\n",
        "        for file in glob.glob(\"/content/SPADE/datasets/Places365/\"+folder+\"/\"+cat+\"*\"):\n",
        "            os.remove(file)\n",
        "            count+=1 \n",
        "print(\"No of files removed:\",count)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of files removed: 100000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}